{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio de Introducción al Procesamiento de Lenguaje Natural 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de las sentencias del corpus del Poder Judicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "corpus = pandas.read_csv(\"corpus_pj.csv\", delimiter=',', skip_blank_lines=True, encoding='utf-8')\n",
    "corpus = corpus.drop_duplicates()\n",
    "sentencias = corpus[\"sentencia\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separar en oraciones y tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "oraciones = []\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "\n",
    "for sentencia in sentencias:\n",
    "    for oracion in sent_detector.tokenize(sentencia):\n",
    "        oraciones.append(oracion)\n",
    "        # oraciones.append(nltk.word_tokenize(oracion))\n",
    "              \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analizar tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "{'lang': 'es', 'words': [{'lemma': u'resultando_que', 'wordform': u'RESULTANDO_QUE', 'tag': u'NP00000', 'prob': 1.0, 'analysis': 1}, {'lemma': u':', 'wordform': u':', 'tag': u'Fd', 'prob': 1.0, 'analysis': 1}, {'lemma': u'1', 'wordform': u'1', 'tag': u'Z', 'prob': 1.0, 'analysis': 1}, {'lemma': u'por', 'wordform': u'Por', 'tag': u'NP00000', 'prob': 1.0, 'analysis': 1}, {'lemma': u'sentencia', 'wordform': u'sentencia', 'tag': u'NCFS000', 'prob': 0.9722863741339492, 'analysis': 3}, {'lemma': u'no', 'wordform': u'No', 'tag': u'NP00000', 'prob': 1.0, 'analysis': 1}, {'lemma': u'.', 'wordform': u'.', 'tag': u'Fp', 'prob': 1.0, 'analysis': 1}]}\n"
     ]
    }
   ],
   "source": [
    "# -*- encoding: utf-8 -*-\n",
    "\n",
    "import sys\n",
    "import json\n",
    "\n",
    "sys.path.append('.../myfreeling/APIs/python')\n",
    "import freeling\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "# Configuracion de freeling\n",
    "# -------------------------\n",
    "FREELINGDIR = \"/usr/local\";\n",
    "DATA = FREELINGDIR+\"/share/freeling/\"\n",
    "LANG = \"es\"\n",
    "\n",
    "freeling.util_init_locale(\"default\")\n",
    "\n",
    "op = freeling.maco_options(\"es\")\n",
    "# # op.set_active_modules(1,1,1,1,1,1,1,1,1,1,0)\n",
    "op.set_data_files( \"\", \n",
    "                   DATA + \"common/punct.dat\",\n",
    "                   DATA + LANG + \"/dicc.src\",\n",
    "                   DATA + LANG + \"/afixos.dat\",\n",
    "                   \"\",\n",
    "                   DATA + LANG + \"/locucions.dat\", \n",
    "                   DATA + LANG + \"/np.dat\",\n",
    "                   DATA + LANG + \"/quantities.dat\",\n",
    "                   DATA + LANG + \"/probabilitats.dat\")\n",
    "op.set_retok_contractions(False)\n",
    "\n",
    "# lg  = freeling.lang_ident(DATA+\"common/lang_ident/ident-few.dat\")\n",
    "mf  = freeling.maco(op)\n",
    "\n",
    "tk  = freeling.tokenizer(DATA+LANG+\"/tokenizer.dat\")\n",
    "sp  = freeling.splitter(DATA+LANG+\"/splitter.dat\")\n",
    "tg  = freeling.hmm_tagger(DATA+LANG+\"/tagger.dat\",1,2)\n",
    "\n",
    "# sen = freeling.senses(DATA+LANG+\"/senses.dat\");\n",
    "# ukb = freeling.ukb(DATA+LANG+\"/ukb.dat\")\n",
    "\n",
    "def tag (sent):\n",
    "    out = {}\n",
    "    lang = \"es\"\n",
    "    l = tk.tokenize(sent)\n",
    "    ls = sp.split(l) # old value 0\n",
    "    ls = mf.analyze(ls)\n",
    "    ls = tg.analyze(ls)\n",
    "    wss = []\n",
    "    for s in ls:\n",
    "        ws = s.get_words()\n",
    "        for w in ws:\n",
    "            an = w.get_analysis()\n",
    "            a = an[0]\n",
    "            wse = dict(wordform =  w.get_form(),\n",
    "                       lemma = a.get_lemma(),\n",
    "                       tag = a.get_tag(),\n",
    "                       prob = a.get_prob(),\n",
    "                       analysis = len(an))\n",
    "            wss.append(wse)\n",
    "    out['words'] = wss\n",
    "    out['lang'] = lang\n",
    "    return out\n",
    "\n",
    "corpus_taggeado = []\n",
    "for s in oraciones[1:3]:\n",
    "    corpus_taggeado.append(tag(s))\n",
    "\n",
    "print len(corpus_taggeado)\n",
    "print corpus_taggeado[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identificar entidades con nombre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
