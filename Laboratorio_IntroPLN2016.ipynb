{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio de Introducción al Procesamiento de Lenguaje Natural 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de las sentencias del corpus del Poder Judicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import nltk\n",
    "\n",
    "corpus = pandas.read_csv(\"corpus_pj.csv\", delimiter=',', skip_blank_lines=True, encoding='utf-8')\n",
    "corpus = corpus.drop_duplicates()\n",
    "sentencias = corpus[\"sentencia\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de Freeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- encoding: utf-8 -*-\n",
    "import sys\n",
    "import json\n",
    "\n",
    "#sys.path.append('/home/kovy/IPLN/FreeLing/APIs/python')\n",
    "import freeling\n",
    "import codecs\n",
    "import re\n",
    "\n",
    "# Configuracion de freeling\n",
    "# -------------------------\n",
    "FREELINGDIR = \"/usr/share\";\n",
    "DATA = FREELINGDIR+\"/freeling/\"\n",
    "LANG = \"es\"\n",
    "\n",
    "freeling.util_init_locale(\"default\")\n",
    "\n",
    "op = freeling.maco_options(\"es\")\n",
    "#op.set_active_modules(1,1,1,1,1,1,1,1,1,1,0)\n",
    "op.set_data_files( \"\", \n",
    "                   DATA + \"common/punct.dat\",\n",
    "                   DATA + LANG + \"/dicc.src\",\n",
    "                   DATA + LANG + \"/afixos.dat\",\n",
    "                   \"\",\n",
    "                   DATA + LANG + \"/locucions.dat\", \n",
    "                   DATA + LANG + \"/np.dat\",\n",
    "                   DATA + LANG + \"/quantities.dat\",\n",
    "                   DATA + LANG + \"/probabilitats.dat\")\n",
    "op.set_retok_contractions(False)\n",
    "\n",
    "# lg  = freeling.lang_ident(DATA+\"common/lang_ident/ident-few.dat\")\n",
    "mf  = freeling.maco(op)\n",
    "\n",
    "tk  = freeling.tokenizer(DATA+LANG+\"/tokenizer.dat\")\n",
    "sp  = freeling.splitter(DATA+LANG+\"/splitter.dat\")\n",
    "tg  = freeling.hmm_tagger(DATA+LANG+\"/tagger.dat\",True,2)\n",
    "tagset = freeling.tagset(DATA+LANG+\"/tagset.dat\")\n",
    "ner = freeling.ner(DATA+LANG+\"/nerc/ner/ner-ab-poor1.dat\")\n",
    "nec = freeling.nec(DATA+LANG+\"/nerc/nec/nec-ab-poor1.dat\")\n",
    "\n",
    "# sen = freeling.senses(DATA+LANG+\"/senses.dat\");\n",
    "# ukb = freeling.ukb(DATA+LANG+\"/ukb.dat\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Funciones auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Función que etiqueta las palabras de una oración \n",
    "def tag (sent):\n",
    "    out = {}\n",
    "    # l = tk.tokenize(sent)\n",
    "    # ls = sp.split(l) # old value 0\n",
    "    ls = sent\n",
    "    ls = mf.analyze(ls)\n",
    "    ls = tg.analyze(ls)\n",
    "    ls = ner.analyze(ls)\n",
    "    ls = nec.analyze(ls)\n",
    "    \n",
    "    wss = []\n",
    "    wss_aux = []\n",
    "    in_contr = False\n",
    "    ws = ls.get_words()\n",
    "    indice_original = 0\n",
    "    for w in ws:\n",
    "        # Obtengo el analisis\n",
    "        an = w.get_analysis()\n",
    "        a = an[0]\n",
    "        wse = dict(wordform =  w.get_form(),\n",
    "                   lemma = a.get_lemma(),\n",
    "                   tag = a.get_tag(),\n",
    "                   prob = a.get_prob(),\n",
    "                   analysis = len(an),\n",
    "                    sent=a,ls=ls, palabras = 0)\n",
    "        \n",
    "        # Chequeo si la palabra es parte de una contraccion\n",
    "        if (indice_original < len(sent)) and ((w.get_form() == sent[indice_original].get_form()) or (sent[indice_original].get_form() in w.get_form())):\n",
    "            if (sent[indice_original].get_form() in w.get_form()):\n",
    "                # Palabra compuesta\n",
    "                while (indice_original < len(sent)) and (sent[indice_original].get_form() in w.get_form()):\n",
    "                    if (contador_palabra==0):\n",
    "                        wse_comp = dict(wordform =  sent[indice_original].get_form(),\n",
    "                               lemma = a.get_lemma(),\n",
    "                               tag = a.get_tag(),\n",
    "                               prob = a.get_prob(),\n",
    "                               analysis = len(an),\n",
    "                               sent=a,ls=ls, palabras = 0)\n",
    "                    else:\n",
    "                        wse_comp = dict(wordform =  sent[indice_original].get_form(),\n",
    "                               lemma = a.get_lemma(),\n",
    "                               tag = a.get_tag() + \"I\",\n",
    "                               prob = a.get_prob(),\n",
    "                               analysis = len(an),\n",
    "                               sent=a,ls=ls, palabras = 0)\n",
    "                    contador_palabra+=1\n",
    "                    wss_comp_aux.append(wse_comp)\n",
    "                    indice_original += 1\n",
    "                indice_original += -1 # corrijo\n",
    "                \n",
    "            indice_original += 1 \n",
    "            if len(wss_comp_aux)>1:\n",
    "                    #si es una palabra compuesta\n",
    "                    for w in wss_comp_aux:\n",
    "                        wss.append(w)\n",
    "                        \n",
    "            if in_contr:\n",
    "                # Termino la contraccion\n",
    "                wss.append(wcont)\n",
    "                wss += wss_aux\n",
    "                wss_aux = []\n",
    "                in_contr = False\n",
    "                \n",
    "            if len(wss_comp_aux)==1:\n",
    "                #si no es una palabra compuesta la agrego\n",
    "                wss.append(wse)\n",
    "            \n",
    "        else:\n",
    "            if (indice_original < len(sent)):\n",
    "                if not in_contr:\n",
    "                    # Palabra original\n",
    "                    if sent[indice_original].get_form() in ['al', 'del']: # contracciones buscadas\n",
    "                        wcont = dict(wordform =  sent[indice_original].get_form(),\n",
    "                                     lemma = '_',\n",
    "                                     tag = '_',\n",
    "                                     prob = 0,\n",
    "                                     analysis = 0,\n",
    "                                     sent=sent[indice_original].get_form(),ls='_', palabras = 1)                  \n",
    "                        indice_original += 1     \n",
    "                        in_contr = True\n",
    "\n",
    "                        # Palabra actual\n",
    "                        wss_aux.append(wse)\n",
    "                    else: # asumo que la palabra encontrada fue agregada por freeling\n",
    "                        wss.append(wse)\n",
    "\n",
    "                else:                \n",
    "                    wcont['palabras'] += 1\n",
    "                    wss_aux.append(wse)\n",
    "            \n",
    "            \n",
    "\n",
    "        \n",
    "    return wss\n",
    "\n",
    "\n",
    "# Mapeo de etiquetas entre la salida de freeling y UPOSTAG\n",
    "tagMapping = [(\"NC\",\"NOUN\"),(\"NP\",\"PROPN\"),(\"A\",\"ADJ\"), (\"PP\",\"PRON\"),(\"D\",\"DET\"),(\"Z\",\"NUM\"),(\"W\",\"ADJ\"),(\"VM\",\"VERB\")\n",
    "              ,(\"V\",\"AUX\"),(\"R\",\"ADV\"),(\"CC\",\"CONJ\"),(\"CS\",\"SCONJ\"),(\"I\",\"INTJ\"),(\"F\",\"PUNCT\"),(\"SP\",\"ADP\"),(\"_\",\"_\")]\n",
    "\n",
    "# Función que mapea las etiquetas de freeling con las de UPOSTAG\n",
    "def tagToStr(tag):\n",
    "    r = [s for t,s in tagMapping if tag.startswith(t)]\n",
    "    return \"??\" + tag + \"??\" if len(r) == 0 else r[0]\n",
    "\n",
    "# Función que convierte la etiqueta en IOB para entidades con nombre\n",
    "def tagToBIO(tag):\n",
    "    if tag == \"NP00SP0\":\n",
    "        return \"B-PER\"\n",
    "    elif tag == \"NP00G00\":\n",
    "        return \"B-LOC\"\n",
    "    elif tag == \"NP00O00\":\n",
    "        return \"B-ORG\"\n",
    "    elif tag == \"NP00V00\":\n",
    "        return \"B-OTR\"\n",
    "    else:\n",
    "        return \"O\"\n",
    "\n",
    "def getLine(sentencia):\n",
    "    return \"{:22}\\t{:25}\\t{:8}\\t{:50}\\t_\\t_\\t_\\t_\\t{}\".format(sentencia['wordform'].encode('utf-8'), \n",
    "                                            sentencia['lemma'].encode('utf-8'), \n",
    "                                            tagToStr(tagset.get_short_tag(sentencia['tag'])), \n",
    "                                            tagset.get_msd_string(sentencia['tag']),\n",
    "                                            tagToBIO(sentencia['tag'])\n",
    "                                           )\n",
    "\n",
    "def getLines(oracion):\n",
    "    i = 1\n",
    "    salida = ''\n",
    "    for palabra in oracion:\n",
    "        if palabra['palabras'] == 0:\n",
    "            salida += '\\n' + str(i) + '\\t' + getLine(palabra)\n",
    "            i += 1\n",
    "        else:\n",
    "            salida += '\\n' + str(i) + '-' + str(i + palabra['palabras'] -1) + '\\t' + getLine(palabra)\n",
    "    return salida\n",
    "    #return \"\\n\".join([str(i) + \"\\t\" + getLine(s) for i, s in enumerate(oracion, start=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Función principal de etiquetado del corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "  \n",
    "corpus_taggeado = []\n",
    "for i, s in enumerate(sentencias[0:1]):       \n",
    "    corpus_taggeado.append(\"# \" + str(i))\n",
    "    l = tk.tokenize(s)\n",
    "    ls = sp.split(l) # old value 0\n",
    "    for oracion in ls:\n",
    "        tagged = tag(oracion)\n",
    "        corpus_taggeado.append(getLines(tagged))\n",
    "        corpus_taggeado.append(\"\\n\")\n",
    "        \n",
    "for l in corpus_taggeado:\n",
    "    print l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for sentencia in sentencias:\n",
    "    l = tk.tokenize(sentencia)\n",
    "    ls = sp.split(l) # old value 0\n",
    "    ls = mf.analyze(ls)\n",
    "    ls = tg.analyze(ls)\n",
    "\n",
    "    ls = ner.analyze(ls)\n",
    "    ls = nec.analyze(ls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "o = freeling.output_conll()\n",
    "print (o.PrintResults(ls))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seleccionador de Sentencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seleccionar_sentencia(nro_sentencia):\n",
    "    sent = sentencias[nro_sentencia]\n",
    "    l = tk.tokenize(sent)\n",
    "    ls = sp.split(l)\n",
    "    for oracion in ls:\n",
    "        tagged = tag(oracion)\n",
    "        corpus_taggeado.append(getLines(tagged))\n",
    "        corpus_taggeado.append(\"\\n\")\n",
    "    for c in corpus_taggeado:\n",
    "        print(c)\n",
    "        \n",
    "def seleccionador_de_sentencias():\n",
    "    num= input(\"Ingrese el número de sentencia deseado: \")\n",
    "    seleccionar_sentencia(int(num))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrucciones para el etiquetado manual\n",
    "Para cada sentencia en el corpus:\n",
    "* Escribir # NUM_SENTENCIA\n",
    "* Escribir cada palabra en una nueva línea, dejando una línea en blanco al final de cada oración\n",
    "* Identificar las contracciones y escribir debajo de cada una, las palabras que la componen (una por línea)\n",
    "* Numerar cada palabra, comenzando desde 1 para cada oración. En el caso de las contracciones se los numera con el rango * de palabras que abarca. La numeración correspondiente se escribe antes de la palabra, seguida de un tabulado.\n",
    "* Para cada palabra escribir los siguientes campos (separados por tabulador):\n",
    "  * LEMMA\n",
    "  * UPOSTAG                   \n",
    "  * FEATS: lista de features morfológicos\n",
    "  * Cuatro campos con barra baja ( _ )\n",
    "  * MISC: identificación de entidades con nombre en el formato IOB (B indica primer palabra de la entidad nombrada, I una palabra interna de la entidad y O una palabra externa a cualquier entidad nombrada) que distinga organizaciones (B-ORG, I-ORG), personas (B-PER, I-PER) y localidades (B-LOC, I-LOC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Problemas Encontrados\n",
    "\n",
    "### Manejo de contracciones\n",
    "Al detectar una contracción durante el análisis de una oración Freeling la transforma en sus componentes y \"olvida\" que provienen de la misma contracción. Por esto, para "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
