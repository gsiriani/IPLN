{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio de Introducción al Procesamiento de Lenguaje Natural 2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de las sentencias del corpus del Poder Judicial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas\n",
    "import nltk\n",
    "\n",
    "corpus = pandas.read_csv(\"corpus_pj.csv\", delimiter=',', skip_blank_lines=True, encoding='utf-8')\n",
    "corpus = corpus.drop_duplicates()\n",
    "sentencias = corpus[\"sentencia\"]\n",
    "\n",
    "oraciones = []\n",
    "\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "\n",
    "sentencia = sentencias[0]\n",
    "for oracion in sent_detector.tokenize(sentencia):\n",
    "    oraciones.append(oracion)\n",
    "    # oraciones.append(nltk.word_tokenize(oracion))\n",
    "\n",
    "\n",
    "# -*- encoding: utf-8 -*-\n",
    "import sys\n",
    "import json\n",
    "\n",
    "sys.path.append('/home/kovy/IPLN/FreeLing/APIs/python')\n",
    "import freeling\n",
    "import codecs\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Configuracion de freeling\n",
    "# -------------------------\n",
    "FREELINGDIR = \"/usr/share\";\n",
    "DATA = FREELINGDIR+\"/freeling/\"\n",
    "LANG = \"es\"\n",
    "\n",
    "freeling.util_init_locale(\"default\")\n",
    "\n",
    "op = freeling.maco_options(\"es\")\n",
    "#op.set_active_modules(1,1,1,1,1,1,1,1,1,1,0)\n",
    "op.set_data_files( \"\", \n",
    "                   DATA + \"common/punct.dat\",\n",
    "                   DATA + LANG + \"/dicc.src\",\n",
    "                   DATA + LANG + \"/afixos.dat\",\n",
    "                   \"\",\n",
    "                   DATA + LANG + \"/locucions.dat\", \n",
    "                   DATA + LANG + \"/np.dat\",\n",
    "                   DATA + LANG + \"/quantities.dat\",\n",
    "                   DATA + LANG + \"/probabilitats.dat\")\n",
    "op.set_retok_contractions(False)\n",
    "\n",
    "# lg  = freeling.lang_ident(DATA+\"common/lang_ident/ident-few.dat\")\n",
    "mf  = freeling.maco(op)\n",
    "\n",
    "tk  = freeling.tokenizer(DATA+LANG+\"/tokenizer.dat\")\n",
    "sp  = freeling.splitter(DATA+LANG+\"/splitter.dat\")\n",
    "tg  = freeling.hmm_tagger(DATA+LANG+\"/tagger.dat\",1,2)\n",
    "tagset = freeling.tagset(DATA+LANG+\"/tagset.dat\")\n",
    "ner = freeling.ner(DATA+LANG+\"/nerc/ner/ner-ab-poor1.dat\")\n",
    "\n",
    "# sen = freeling.senses(DATA+LANG+\"/senses.dat\");\n",
    "# ukb = freeling.ukb(DATA+LANG+\"/ukb.dat\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tag (sent):\n",
    "    out = {}\n",
    "    lang = \"es\"\n",
    "    l = tk.tokenize(sent)\n",
    "    ls = sp.split(l) # old value 0\n",
    "    ls = mf.analyze(ls)\n",
    "    ls = tg.analyze(ls)\n",
    "    ls = ner.analyze(ls)\n",
    "    wss = []\n",
    "    for s in ls:\n",
    "        ws = s.get_words()\n",
    "        for w in ws:\n",
    "            an = w.get_analysis()\n",
    "            a = an[0]\n",
    "            wse = dict(wordform =  w.get_form(),\n",
    "                       lemma = a.get_lemma(),\n",
    "                       tag = a.get_tag(),\n",
    "                       prob = a.get_prob(),\n",
    "                       analysis = len(an),\n",
    "                        sent=a)\n",
    "            wss.append(wse)\n",
    "    out['words'] = wss\n",
    "    out['lang'] = lang\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos=noun|type=proper\n",
      "NP\n"
     ]
    }
   ],
   "source": [
    "print(corpus_taggeado[0]['words'][0]['tag']))\n",
    "print(tagset.get_short_tag(corpus_taggeado[0]['words'][0]['tag']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tagMapping = [(\"NC\",\"NOUN\"),(\"NP\",\"PROPN\"),(\"A\",\"ADJ\"), (\"PP\",\"PRON\"),(\"D\",\"DET\"),(\"Z\",\"NUM\"),(\"W\",\"ADJ\"),(\"VM\",\"VERB\")\n",
    "              ,(\"V\",\"AUX\"),(\"R\",\"ADV\"),(\"CC\",\"CONJ\"),(\"CS\",\"SCONJ\"),(\"I\",\"INTJ\"),(\"F\",\"PUNCT\"),(\"SP\",\"ADP\")]\n",
    "              \n",
    "def tagToStr(tag):\n",
    "    return [s for t,s in tagMapping if tag.startswith(t)][0]\n",
    "\n",
    "def getLine(sentencia):\n",
    "    return \"{:22}\\t{:25}{:8}{}\".format(sentencia['wordform'], sentencia['lemma'], \n",
    "                                        tagToStr(tagset.get_short_tag(sentencia['tag'])),\n",
    "                                       tagset.get_msd_string(sentencia['tag']))\n",
    "def getLines(sentencias):\n",
    "    return \"\\n\".join([str(i) + \" \" + getLine(s) for i, s in enumerate(sentencias)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 RESULTANDO_QUE        \tresultando_que           PROPN   pos=noun|type=proper\n",
      "1 :                     \t:                        PUNCT   pos=punctuation|type=colon\n",
      "2 1                     \t1                        NUM     pos=number\n",
      "3 Por                   \tpor                      PROPN   pos=noun|type=proper\n",
      "4 sentencia             \tsentencia                NOUN    pos=noun|type=common|gen=feminine|num=singular\n",
      "5 No                    \tno                       PROPN   pos=noun|type=proper\n",
      "6 .                     \t.                        PUNCT   pos=punctuation|type=period\n",
      "0 188                   \t188                      NUM     pos=number\n",
      "1 de                    \tde                       ADP     pos=adposition|type=preposition\n",
      "2 el                    \tel                       DET     pos=determiner|type=article|gen=masculine|num=singular\n",
      "3 Tribunal_de_Apelaciones\ttribunal_de_apelaciones  PROPN   pos=noun|type=proper\n",
      "4 en                    \ten                       ADP     pos=adposition|type=preposition\n",
      "5 lo                    \tel                       DET     pos=determiner|type=article|num=singular\n",
      "6 Civil_de_Primer_Turno \tcivil_de_primer_turno    PROPN   pos=noun|type=proper\n",
      "7 ,                     \t,                        PUNCT   pos=punctuation|type=comma\n",
      "8 de                    \tde                       ADP     pos=adposition|type=preposition\n",
      "9 9_de_agosto_de_2006   \t[??:9/8/2006:??.??:??]   ADJ     pos=date\n",
      "10 (                     \t(                        PUNCT   pos=punctuation|type=parenthesis|punctenclose=open\n",
      "11 fs                    \tfs                       NOUN    pos=noun|type=common|gen=masculine|num=plural\n",
      "12 .                     \t.                        PUNCT   pos=punctuation|type=period\n"
     ]
    }
   ],
   "source": [
    "  \n",
    "corpus_taggeado = []\n",
    "for i, s in enumerate(oraciones[1:3]):\n",
    "    tagged = tag(s)\n",
    "    corpus_taggeado.append(tagged)\n",
    "    print(getLines(tagged['words']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-67b903d0cb42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "a = ls[1]\n",
    "b = a.get_words()\n",
    "b.get_tag()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
